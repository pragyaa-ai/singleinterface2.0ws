import 'dotenv/config';
import http from 'http';
import WebSocket, { WebSocketServer } from 'ws';
import fs from 'fs';
import path from 'path';
import { int16ArrayToBase64, ensureInt16Array, upsample8kTo24k, downsample24kTo8k, initializeResamplers } from './audio';

// üîÑ DECOUPLED ARCHITECTURE: Agent processing moved to async service
// Telephony service now focuses only on call handling and transcript collection

interface OzonetelMediaPacket {
  event: string;
  type: string;
  ucid: string;
  data?: {
    samples: number[];
    bitsPerSample: number;
    sampleRate: number;
    channelCount: number;
    numberOfFrames: number;
    type: 'data';
  };
}

interface QuestionAnswerPair {
  question: string;
  answer: string;
  questionTimestamp: number;
  answerTimestamp: number;
  duration: number;
  dataType?: 'full_name' | 'car_model' | 'email_id';
  reattempts: number;
}

interface CallAnalytics {
  callStartTime: number;
  callEndTime?: number;
  questionAnswerPairs: QuestionAnswerPair[];
  parametersAttempted: Set<string>;
  parametersCaptured: Set<string>;
  dropOffPoint?: {
    lastEvent: string;
    timestamp: number;
    context: string;
  };
  currentSpeechStart?: number;
  currentQuestionStart?: number;
}

interface TranscriptEntry {
  timestamp: string;
  speaker: 'user' | 'assistant';
  text: string;
  confidence?: number;
  event_type?: string;
}

// NEW: Enhanced data point status tracking
interface DataPointStatus {
  value?: string;
  status: 'verified' | 'captured' | 'needs_to_be_validated' | 'not_captured';
  confidence?: number;
  attempts: number;
  timestamps: {
    first_attempt?: number;
    last_attempt?: number;
    verified_at?: number;
  };
}

interface Session {
  ucid: string;
  client: WebSocket;
  openaiWs: WebSocket;
  receivedFirstPacket: boolean;
  inputFrameBuffer: number[][]; // queue of sample arrays
  salesData: {
    full_name?: string;
    car_model?: string;
    email_id?: string;
    verified: Set<string>;
    processing_status?: 'pending' | 'processing' | 'completed' | 'failed';
    // NEW: Enhanced data point tracking
    dataPoints?: {
      full_name: DataPointStatus;
      car_model: DataPointStatus;
      email_id: DataPointStatus;
    };
  };
  transcripts: string[]; // Keep for backward compatibility
  fullTranscript: TranscriptEntry[]; // üîÑ NEW: Rich transcript with timestamps
  lastCapturedData?: string;
  callAnalytics?: CallAnalytics;
}

const port = Number(process.env.TELEPHONY_WS_PORT || 8080);
const host = process.env.TELEPHONY_WS_HOST || '0.0.0.0';

const server = http.createServer((req, res) => {
  // Handle Ozonetel XML requests for /getXML_SI/
  if (req.url?.startsWith('/getXML_SI/')) {
    console.log(`üìû Ozonetel XML request: ${req.method} ${req.url}`);
    
    // Parse query parameters to get call details
    const url = new URL(req.url, `http://${req.headers.host}`);
    const outbound_sid = url.searchParams.get('outbound_sid');
    const event = url.searchParams.get('event');
    const cid = url.searchParams.get('cid');
    
    console.log(`üìû Call details: sid=${outbound_sid}, event=${event}, cid=${cid}`);
    
    // Return XML response pointing to our WebSocket endpoint
    const wsUrl = process.env.TELEPHONY_WS_URL || 'wss://ws-singleinterfacews.pragyaa.ai/ws';
    const sipId = process.env.TELEPHONY_SIP_ID || outbound_sid || 'UNKNOWN';
    
    const xml = `<response><stream is_sip='true' url='${wsUrl}'>${sipId}</stream></response>`;
    
    res.writeHead(200, { 
      'Content-Type': 'text/xml; charset=utf-8',
      'Cache-Control': 'no-store'
    });
    res.end(xml);
    
    console.log(`üìû XML response sent: ${xml}`);
  } else {
    // Handle other requests
    res.writeHead(404, { 'Content-Type': 'text/plain' });
    res.end('Not Found');
  }
});
const wss = new WebSocketServer({ server, path: '/ws' });

const sessions = new Map<string, Session>();

// üîÑ ENHANCED: Ensure all data directories exist for decoupled architecture
const dataDir = path.join(process.cwd(), 'data', 'calls');
const transcriptsDir = path.join(process.cwd(), 'data', 'transcripts');
const processingDir = path.join(process.cwd(), 'data', 'processing');
const resultsDir = path.join(process.cwd(), 'data', 'results');

[dataDir, transcriptsDir, processingDir, resultsDir].forEach(dir => {
  if (!fs.existsSync(dir)) {
    fs.mkdirSync(dir, { recursive: true });
    console.log(`üìÅ Created directory: ${dir}`);
  }
});

// üéØ Transfer Tool - Hands off call to dealer after data collection
const telephonySDKTools: any[] = [
  {
    type: "function" as const,
    name: "transfer_call",
    description: "CRITICAL: Call this function to transfer the call to a Mahindra dealer. REQUIRED after collecting all 3 customer details (name, car model, email) OR if customer asks to speak with a human. You MUST call this function, not just say you will transfer.",
    parameters: {
      type: "object",
      properties: {
        reason: {
          type: "string",
          enum: ["data_collected", "customer_request", "technical_issue"],
          description: "Why you are transferring: 'data_collected' after getting all 3 details, 'customer_request' if they ask for human, 'technical_issue' if problem occurs"
        }
      },
      required: ["reason"]
    }
  }
];

// üîÑ TRANSFER HANDLER
// Executes Waybeo transfer API to hand off call to dealer

async function executeWaybeoTransfer(callId: string, reason: string): Promise<boolean> {
  const waybeoUrl = process.env.WAYBEO_TRANSFER_URL || 'https://pbx-uat.waybeo.com/bot-call';
  const authToken = process.env.WAYBEO_AUTH_TOKEN;

  if (!authToken) {
    console.error(`[${callId}] ‚ùå WAYBEO_AUTH_TOKEN not configured - cannot transfer call`);
    return false;
  }

  const payload = {
    command: "transfer",
    callId: callId
  };

  console.log(`[${callId}] üîÑ Calling Waybeo Transfer API...`);
  console.log(`[${callId}] üì§ Transfer payload:`, JSON.stringify(payload, null, 2));

  try {
    const response = await fetch(waybeoUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${authToken}`
      },
      body: JSON.stringify(payload)
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`[${callId}] ‚ùå Waybeo transfer failed: HTTP ${response.status} - ${errorText}`);
      return false;
    }

    const result = await response.json();
    console.log(`[${callId}] ‚úÖ Waybeo transfer successful:`, result);
    return true;

  } catch (error) {
    console.error(`[${callId}] ‚ùå Waybeo transfer error:`, error);
    return false;
  }
}

async function handleTransferCall(session: Session, reason: string) {
  const ucid = session.ucid;
  
  console.log(`[${ucid}] üîÑ Transfer requested - Reason: ${reason}`);
  
  // Save any transcript data collected so far
  saveTranscriptForProcessing(session);
  console.log(`[${ucid}] üíæ Transcript saved before transfer`);
  
  // ‚è∞ CRITICAL: Add delay to allow agent to finish speaking before transferring
  // The function call happens while agent is still generating speech
  // We need to wait for speech to complete before calling Waybeo API
  const speechCompletionDelay = 5000; // 5 seconds - allows complete message to be heard
  console.log(`[${ucid}] ‚è≥ Waiting ${speechCompletionDelay}ms for agent to complete transfer message...`);
  
  await new Promise(resolve => setTimeout(resolve, speechCompletionDelay));
  
  console.log(`[${ucid}] ‚úÖ Speech completion wait finished - executing transfer now`);
  
  // Execute transfer API call
  const transferSuccess = await executeWaybeoTransfer(ucid, reason);
  
  if (transferSuccess) {
    console.log(`[${ucid}] ‚úÖ Transfer API call successful`);
    console.log(`[${ucid}] üîç OBSERVING: Waiting to see if Waybeo sends WebSocket close event...`);
    // üß™ TESTING: We're NOT manually closing connections here
    // We want to observe if Waybeo sends a close event after transfer
    // If they don't, we'll request this feature from them
  } else {
    console.error(`[${ucid}] ‚ùå Transfer API failed - call will continue`);
  }
}

// üîÑ NEW: Save complete transcript for async processing
function saveTranscriptForProcessing(session: Session) {
  const ucid = session.ucid;
  const timestamp = Date.now();
  
  // Create transcript file data
  const transcriptData = {
    call_id: ucid,
    timestamp: new Date().toISOString(),
    call_start_time: session.callAnalytics?.callStartTime || timestamp,
    call_end_time: timestamp,
    call_duration: session.callAnalytics?.callStartTime ? 
      timestamp - session.callAnalytics.callStartTime : 0,
    
    // Full conversation with timestamps
    conversation: session.fullTranscript,
    
    // Backward compatibility - simple transcript array
    simple_transcripts: session.transcripts,
    
    // Current sales data (partial or complete)
    current_sales_data: {
      full_name: session.salesData.full_name || null,
      car_model: session.salesData.car_model || null,
      email_id: session.salesData.email_id || null,
      verified_fields: Array.from(session.salesData.verified || []),
      processing_status: 'pending'
    },
    
    // Call analytics
    analytics: session.callAnalytics ? {
      total_exchanges: session.fullTranscript.length,
      user_messages: session.fullTranscript.filter(t => t.speaker === 'user').length,
      assistant_messages: session.fullTranscript.filter(t => t.speaker === 'assistant').length,
      question_answer_pairs: session.callAnalytics.questionAnswerPairs?.length || 0,
      parameters_attempted: Array.from(session.callAnalytics.parametersAttempted || []),
      parameters_captured: Array.from(session.callAnalytics.parametersCaptured || [])
    } : null
  };
  
  // Save transcript file
  const transcriptFilename = `call_${ucid}_${timestamp}_transcript.json`;
  const transcriptPath = path.join(transcriptsDir, transcriptFilename);
  
  try {
    fs.writeFileSync(transcriptPath, JSON.stringify(transcriptData, null, 2));
    console.log(`[${ucid}] üìÑ Transcript saved: ${transcriptFilename}`);
    
    // Create processing queue entry
    const queueData = {
      call_id: ucid,
      transcript_file: transcriptFilename,
      created_at: new Date().toISOString(),
      status: 'pending',
      priority: 'normal'
    };
    
    const queueFilename = `call_${ucid}_${timestamp}_queue.json`;
    const queuePath = path.join(processingDir, queueFilename);
    
    fs.writeFileSync(queuePath, JSON.stringify(queueData, null, 2));
    console.log(`[${ucid}] üìã Processing queue entry created: ${queueFilename}`);
    
    // Update session status
    session.salesData.processing_status = 'pending';
    
    return { transcriptFile: transcriptFilename, queueFile: queueFilename };
    
  } catch (error) {
    console.error(`[${ucid}] ‚ùå Failed to save transcript:`, error);
    return null;
  }
}

// Save sales data to local file
function saveSalesDataToFile(session: Session) {
  const { full_name, car_model, email_id } = session.salesData;
  const ucid = session.ucid;
  
  // üÜï NEW: Enhanced call data with comprehensive analytics
  const callData = {
    ucid: ucid,
    timestamp: new Date().toISOString(),
    
    // Sales Data
    salesData: {
      full_name: full_name || 'Not captured',
      car_model: car_model || 'Not captured', 
      email_id: email_id || 'Not captured',
      status: (full_name && car_model && email_id) ? 'Complete' : 'Partial'
    },
    
    // üÜï NEW: Call Analytics
    callAnalytics: session.callAnalytics ? {
      callDuration: session.callAnalytics.callEndTime ? 
        session.callAnalytics.callEndTime - session.callAnalytics.callStartTime : 
        Date.now() - session.callAnalytics.callStartTime,
      
      parametersAttempted: Array.from(session.callAnalytics.parametersAttempted),
      parametersCaptured: Array.from(session.callAnalytics.parametersCaptured),
      
      questionAnswerPairs: session.callAnalytics.questionAnswerPairs.map(qa => ({
        question: qa.question,
        answer: qa.answer,
        duration: qa.duration,
        dataType: qa.dataType,
        reattempts: qa.reattempts,
        timestamp: new Date(qa.questionTimestamp).toISOString()
      })),
      
      totalQuestions: session.callAnalytics.questionAnswerPairs.length,
      averageResponseTime: session.callAnalytics.questionAnswerPairs.length > 0 ?
        session.callAnalytics.questionAnswerPairs.reduce((sum, qa) => sum + qa.duration, 0) / 
        session.callAnalytics.questionAnswerPairs.length : 0,
      
      dropOffPoint: session.callAnalytics.dropOffPoint || null
    } : null,
    
    // Conversation Context
    transcripts: session.transcripts || [],
    totalTranscripts: session.transcripts?.length || 0,
    
    // Legacy field for backward compatibility
    call_duration: session.callAnalytics?.callEndTime ? 
      session.callAnalytics.callEndTime - session.callAnalytics.callStartTime : 
      Date.now() - (session.callAnalytics?.callStartTime || Date.now())
  };
  
  const filename = `call_${ucid}_${Date.now()}.json`;
  const filepath = path.join(dataDir, filename);
  
  try {
    fs.writeFileSync(filepath, JSON.stringify(callData, null, 2));
    console.log(`[${ucid}] üíæ Sales data saved to: ${filename}`);
  } catch (error) {
    console.error(`[${ucid}] ‚ùå Failed to save data:`, error);
  }
}

// Check if all required data is collected
function parseAgentOutput(output: string): { full_name?: string, car_model?: string, email_id?: string } | null {
  if (!output) return null;
  
  const result: { full_name?: string, car_model?: string, email_id?: string } = {};
  
  // First try to parse structured JSON if the agent returned it
  try {
    // Look for JSON in the output
    const jsonMatch = output.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      const parsed = JSON.parse(jsonMatch[0]);
      if (parsed.full_name) result.full_name = parsed.full_name;
      if (parsed.car_model) result.car_model = parsed.car_model;
      if (parsed.email_id || parsed.email) result.email_id = parsed.email_id || parsed.email;
      
      // If we found data in JSON, return it
      if (Object.keys(result).length > 0) {
        console.log('üìä Parsed structured JSON from agent:', result);
        return result;
      }
    }
  } catch (e) {
    // Fall back to regex parsing
  }
  
  // Extract full name with improved validation
  const namePatterns = [
    /(?:Full Name|Name):\s*([A-Za-z\s.''-]{2,30})(?:\s*\n|$)/i,
    /(?:full name|name):\s*([A-Za-z\s.''-]{2,30})(?:\s*\n|$)/i,
    /- Full Name:\s*([A-Za-z\s.''-]{2,30})(?:\s*\n|$)/i,
    /\"full_name\":\s*\"([A-Za-z\s.''-]{2,30})\"/i
  ];
  
  for (const pattern of namePatterns) {
    const nameMatch = output.match(pattern);
    if (nameMatch && nameMatch[1].trim() && 
        nameMatch[1].trim() !== 'Not captured' && 
        nameMatch[1].trim().length > 1 &&
        /^[A-Za-z\s.''-]+$/.test(nameMatch[1].trim())) {
      result.full_name = nameMatch[1].trim();
      break;
    }
  }
  
  // Extract email ID with stricter validation
  const emailPatterns = [
    /(?:Email ID|Email):\s*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,})/i,
    /(?:email id|email):\s*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,})/i,
    /- Email ID:\s*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,})/i,
    /\"email_id\":\s*\"([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,})\"/i
  ];
  
  for (const pattern of emailPatterns) {
    const emailMatch = output.match(pattern);
    if (emailMatch && emailMatch[1] && emailMatch[1].includes('@')) {
      result.email_id = emailMatch[1];
      break;
    }
  }
  
  // Extract car model with better filtering
  const carPatterns = [
    /(?:Car Model|Car|Vehicle):\s*([A-Za-z0-9\s\-._]{2,40})(?:\s*\n|$)/i,
    /(?:car model|car|vehicle):\s*([A-Za-z0-9\s\-._]{2,40})(?:\s*\n|$)/i,
    /- Car Model:\s*([A-Za-z0-9\s\-._]{2,40})(?:\s*\n|$)/i,
    /\"car_model\":\s*\"([A-Za-z0-9\s\-._]{2,40})\"/i
  ];
  
  for (const pattern of carPatterns) {
    const carMatch = output.match(pattern);
    if (carMatch && carMatch[1].trim() && 
        carMatch[1].trim() !== 'Not captured' && 
        carMatch[1].trim().length > 1 &&
        // Filter out obvious non-car-model responses
        !['yes', 'no', 'ok', 'hello', 'thanks', 'thank you'].includes(carMatch[1].trim().toLowerCase())) {
      result.car_model = carMatch[1].trim();
      break;
    }
  }
  
  // Return result only if we found at least one piece of valid data
  console.log('üìä Parsed agent output:', result);
  return Object.keys(result).length > 0 ? result : null;
}

// REMOVED: checkDataCompletion function - no real-time data validation
// Data validation happens in queue processor after call ends

async function createOpenAIConnection(ucid: string): Promise<WebSocket> {
  const apiKey = process.env.OPENAI_API_KEY as string;
  if (!apiKey) throw new Error('OPENAI_API_KEY not set');

  // Configurable model - set VOICEAGENT_MODEL env var to switch models
  // Options: gpt-realtime (standard/default for testing), gpt-4o-mini-realtime-preview-2024-12-17 (mini for production)
  const model = process.env.VOICEAGENT_MODEL || 'gpt-realtime';
  console.log(`[${ucid}] ü§ñ Using model: ${model}`);

  return new Promise((resolve, reject) => {
    const openaiWs = new WebSocket(`wss://api.openai.com/v1/realtime?model=${model}`, {
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'OpenAI-Beta': 'realtime=v1'
      }
    });

    openaiWs.on('open', () => {
      console.log(`[${ucid}] Connected to VoiceAgent Realtime API`);
      
      // Configure session for Spotlight agent behavior with SDK tools
      const sessionConfig = {
        type: 'session.update',
        session: {
          input_audio_format: 'pcm16',
          output_audio_format: 'pcm16',
          voice: 'coral',
          input_audio_transcription: { model: 'whisper-1' },
          turn_detection: {
            type: 'server_vad',
            threshold: 0.5,
            prefix_padding_ms: 300,
            silence_duration_ms: 300
          },
          tools: telephonySDKTools,
          tool_choice: "auto",
          temperature: 0.7,
          instructions: `# üö® CRITICAL: Tool Calling Protocol
**YOU MUST CALL THE transfer_call FUNCTION - NOT JUST SAY IT**

## When to Call transfer_call:
1. **After collecting all 3 details** (Name + Car Model + Email) ‚Üí Call transfer_call with reason "data_collected"
2. **If customer requests to speak with dealer/human** ‚Üí Call transfer_call with reason "customer_request"

## Transfer Sequence (CRITICAL - DO THIS IMMEDIATELY AFTER COLLECTING 3RD DETAIL):
**AS SOON AS you receive the 3rd data point (email), DO NOT ask for confirmation:**

1. **IMMEDIATELY**: Say this COMPLETE transfer message: "Thank you for all the details. Let me transfer you to a Mahindra dealer closest to you.............. Please hold on."
2. **THEN IMMEDIATELY**: Call transfer_call function with {"reason": "data_collected"}

DO NOT:
- ‚ùå Ask "Is this correct?" after email
- ‚ùå Wait for user confirmation
- ‚ùå Say anything else before transfer

YOU MUST:
- ‚úÖ Say the COMPLETE transfer message
- ‚úÖ Call transfer_call IMMEDIATELY after saying it
- ‚úÖ Do this RIGHT AFTER collecting the 3rd data point

---

# CRITICAL: Multilingual Support with English Start
You support the following languages: Hindi, English, Marathi, Telugu, Tamil, Malayalam

## Language Switching Protocol:
- **START IN ENGLISH**: ALWAYS begin the conversation with English greeting
- **Automatic Language Detection**: Listen to the customer's response and detect their language
- **Auto-Switch**: If customer responds in Hindi/Marathi/Telugu/Tamil/Malayalam, IMMEDIATELY switch to that language
- **Stay in Detected Language**: Continue the entire conversation in the detected language
- **Manual Switch**: If customer explicitly requests a language change, switch immediately

## Opening Greeting (MANDATORY - Always in English):
ALWAYS start with: "Namaskar.............. Welcome to Dee Emm Mahindra dealer. How may I help you today?"

## Language Switching After First Response:
After the customer responds to your English greeting, detect their language and switch:
- If customer speaks **Hindi**: Switch to Hindi for all remaining conversation
- If customer speaks **Marathi**: Switch to Marathi for all remaining conversation
- If customer speaks **Telugu**: Switch to Telugu for all remaining conversation
- If customer speaks **Tamil**: Switch to Tamil for all remaining conversation
- If customer speaks **Malayalam**: Switch to Malayalam for all remaining conversation
- If customer continues in **English**: Stay in English

## Language-Specific Responses (After switching):
- **Hindi**: "‡§ú‡•Ä ‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤, ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡•Ç‡§Ç‡§ó‡•Ä‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡§æ ‡§®‡§æ‡§Æ ‡§¨‡§§‡§æ‡§è‡§Ç?"
- **Marathi**: "‡§®‡§ï‡•ç‡§ï‡•Ä‡§ö, ‡§Æ‡•Ä ‡§§‡•Å‡§Æ‡§ö‡•Ä ‡§Æ‡§¶‡§§ ‡§ï‡§∞‡•á‡§®‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§§‡•Å‡§Æ‡§ö‡•á ‡§®‡§æ‡§µ ‡§∏‡§æ‡§Ç‡§ó‡§æ?"
- **Telugu**: "‡∞§‡∞™‡±ç‡∞™‡∞ï‡±Å‡∞Ç‡∞°‡∞æ, ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞®‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞Ç‡∞°‡∞ø?"
- **Tamil**: "‡Æ®‡Æø‡Æö‡Øç‡Æö‡ÆØ‡ÆÆ‡Ææ‡Æï, ‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æâ‡Æ§‡Æµ‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. ‡Æ§‡ÆØ‡Æµ‡ØÅ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øà ‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç?"
- **Malayalam**: "‡¥§‡µÄ‡µº‡¥ö‡µç‡¥ö‡¥Ø‡¥æ‡¥Ø‡µÅ‡¥Ç, ‡¥û‡¥æ‡µª ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÜ ‡¥∏‡¥π‡¥æ‡¥Ø‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Ç. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥™‡µá‡¥∞‡µç ‡¥™‡¥±‡¥Ø‡µÇ?"
- **English**: "Sure, I'll be happy to help. May I know your name please?"

## Transfer Messages (EXACT phrases to use in each language):
After collecting all 3 details, say the COMPLETE message in customer's language:
- **English**: "Thank you for all the details. Let me transfer you to a Mahindra dealer closest to you.............. Please hold on."
- **Hindi**: "‡§∏‡§≠‡•Ä ‡§µ‡§ø‡§µ‡§∞‡§£‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•ã ‡§Ü‡§™‡§ï‡•á ‡§∏‡§¨‡§∏‡•á ‡§®‡§ú‡§¶‡•Ä‡§ï‡•Ä ‡§Æ‡§π‡§ø‡§Ç‡§¶‡•ç‡§∞‡§æ ‡§°‡•Ä‡§≤‡§∞ ‡§∏‡•á ‡§ú‡•ã‡§°‡§º‡§§‡•Ä ‡§π‡•Ç‡§Ç.............. ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•ç‡§∞‡§§‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡§∞‡•á‡§Ç‡•§"
- **Marathi**: "‡§∏‡§∞‡•ç‡§µ ‡§§‡§™‡§∂‡•Ä‡§≤‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•§ ‡§Æ‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§§‡•Å‡§Æ‡§ö‡•ç‡§Ø‡§æ ‡§ú‡§µ‡§≥‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§π‡§ø‡§Ç‡§¶‡•ç‡§∞‡§æ ‡§°‡•Ä‡§≤‡§∞‡§∂‡•Ä ‡§ú‡•ã‡§°‡§§‡•á ‡§Ü‡§π‡•á.............. ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•ç‡§∞‡§§‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡§∞‡§æ‡•§"
- **Telugu**: "‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞µ‡∞ø‡∞µ‡∞∞‡∞æ‡∞≤‡∞ï‡±Å ‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å‡•§ ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡∞ø‡∞Æ‡±ç‡∞Æ‡∞≤‡±ç‡∞®‡∞ø ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞¶‡∞ó‡±ç‡∞ó‡∞∞‡∞ó‡∞æ ‡∞â‡∞®‡±ç‡∞® ‡∞Æ‡∞π‡±Ä‡∞Ç‡∞¶‡±ç‡∞∞‡∞æ ‡∞°‡±Ä‡∞≤‡∞∞‡±ç‚Äå‡∞ï‡∞ø ‡∞ï‡∞®‡±Ü‡∞ï‡±ç‡∞ü‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.............. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞µ‡±á‡∞ö‡∞ø ‡∞â‡∞Ç‡∞°‡∞Ç‡∞°‡∞ø‡•§"
- **Tamil**: "‡ÆÖ‡Æ©‡Øà‡Æ§‡Øç‡Æ§‡ØÅ ‡Æµ‡Æø‡Æµ‡Æ∞‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æ®‡Æ©‡Øç‡Æ±‡Æø‡•§ ‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÖ‡Æ∞‡ØÅ‡Æï‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥ ‡ÆÆ‡Æπ‡Æø‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡Ææ ‡Æü‡ØÄ‡Æ≤‡Æ∞‡Æø‡Æü‡ÆÆ‡Øç ‡Æá‡Æ£‡Øà‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç.............. ‡Æ§‡ÆØ‡Æµ‡ØÅ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æï‡Ææ‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç‡•§"
- **Malayalam**: "‡¥é‡¥≤‡µç‡¥≤‡¥æ ‡¥µ‡¥ø‡¥∂‡¥¶‡¥æ‡¥Ç‡¥∂‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µÅ‡¥Ç ‡¥®‡¥®‡µç‡¥¶‡¥ø‡•§ ‡¥û‡¥æ‡µª ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÜ ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Ö‡¥ü‡µÅ‡¥§‡µç‡¥§‡µÅ‡¥≥‡µç‡¥≥ ‡¥Æ‡¥π‡µÄ‡¥®‡µç‡¥¶‡µç‡¥∞ ‡¥°‡µÄ‡¥≤‡¥±‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥¨‡¥®‡µç‡¥ß‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥Ø‡¥æ‡¥£‡µç.............. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥ï‡¥æ‡¥§‡µç‡¥§‡¥ø‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÇ‡•§"

# Personality and Tone
## Identity
You are a North Indian female sales assistant working at Dee Emm Mahindra Dealer. You speak politely and clearly with a distinctly Indian-English accent and Indian pronunciation. Your voice is gentle, warm, and respectful ‚Äî like a caring Indian woman speaking naturally in a real conversation. You specialize in helping customers with Mahindra cars at Dee Emm Mahindra Dealer.
You maintain a slightly faster and more engaging speaking pace from the very beginning to the end of the conversation, while always remaining clear, polite, and respectful.

## Task
You need to collect the *Name, Car model interested in, and Email* from the customer.
- Always collect all three details in the conversation.
- After the introduction, you may ask them in *any order* ‚Äî not necessarily Name ‚Üí Car Model ‚Üí Email.
- Example: Sometimes start with the name, sometimes ask about the car model first, or sometimes begin with email. Shuffle the order randomly so it feels conversational and not scripted.
- Use different conversational variations for asking questions.
- *Capture only what the customer says ‚Äî exactly, same to same.*
- Do not assume, infer, normalize, or add anything that is not explicitly spoken by the customer.
- Confirm every detail word-for-word, in clear Indian accent and pronunciation.
- If a correction is given, always apologize politely before restating the corrected detail.
*Important:* You only handle Mahindra car enquiries. If a caller asks about any other brand, politely explain that Dee Emm is a Mahindra dealer and request their interest in a Mahindra vehicle instead.

## Question Variations (Multilingual - Use randomly in customer's language):

### Hindi:
- **Name**: "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡§æ ‡§™‡•Ç‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§¨‡§§‡§æ‡§è‡§Ç?" / "‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à ‡§ú‡•Ä?" / "‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ú‡§æ‡§® ‡§∏‡§ï‡§§‡•Ä ‡§π‡•Ç‡§Ç?"
- **Car Model**: "‡§Ü‡§™ ‡§ï‡•å‡§® ‡§∏‡§æ ‡§Æ‡§π‡§ø‡§Ç‡§¶‡•ç‡§∞‡§æ ‡§ï‡§æ‡§∞ ‡§Æ‡•â‡§°‡§≤ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç?" / "‡§Ü‡§™ ‡§ï‡§ø‡§∏ ‡§Æ‡§π‡§ø‡§Ç‡§¶‡•ç‡§∞‡§æ ‡§ó‡§æ‡§°‡§º‡•Ä ‡§Æ‡•á‡§Ç ‡§á‡§Ç‡§ü‡§∞‡•á‡§∏‡•ç‡§ü‡•á‡§° ‡§π‡•à‡§Ç?"
- **Email**: "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡§æ ‡§à‡§Æ‡•á‡§≤ ‡§Ü‡§à‡§°‡•Ä ‡§¨‡§§‡§æ‡§è‡§Ç?" / "‡§Ü‡§™‡§ï‡§æ ‡§à‡§Æ‡•á‡§≤ ‡§è‡§°‡•ç‡§∞‡•á‡§∏ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?"

### English:
- **Name**: "May I know your full name, please?" / "What is your good name, please?"
- **Car Model**: "Which Mahindra car model are you interested in?" / "May I know the Mahindra model you have in mind?"
- **Email**: "Could you please share your email ID with me?" / "What would be your email address?"

### Marathi:
- **Name**: "‡§ï‡•É‡§™‡§Ø‡§æ ‡§§‡•Å‡§Æ‡§ö‡•á ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§®‡§æ‡§µ ‡§∏‡§æ‡§Ç‡§ó‡§æ?" / "‡§§‡•Å‡§Æ‡§ö‡•á ‡§®‡§æ‡§µ ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?"
- **Car Model**: "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§ï‡•ã‡§£‡§§‡•á ‡§Æ‡§π‡§ø‡§Ç‡§¶‡•ç‡§∞‡§æ ‡§ï‡§æ‡§∞ ‡§Æ‡•â‡§°‡•á‡§≤ ‡§π‡§µ‡•á ‡§Ü‡§π‡•á?" / "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§ï‡•ã‡§£‡§§‡•ç‡§Ø‡§æ ‡§Æ‡§π‡§ø‡§Ç‡§¶‡•ç‡§∞‡§æ ‡§ó‡§æ‡§°‡•Ä‡§§ ‡§∞‡§∏ ‡§Ü‡§π‡•á?"
- **Email**: "‡§ï‡•É‡§™‡§Ø‡§æ ‡§§‡•Å‡§Æ‡§ö‡§æ ‡§à‡§Æ‡•á‡§≤ ‡§Ü‡§Ø‡§°‡•Ä ‡§¶‡•ç‡§Ø‡§æ?" / "‡§§‡•Å‡§Æ‡§ö‡§æ ‡§à‡§Æ‡•á‡§≤ ‡§Ö‡•Ö‡§°‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?"

### Telugu:
- **Name**: "‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Æ‡±Ä ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞™‡±á‡∞∞‡±Å ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞Ç‡∞°‡∞ø?" / "‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?"
- **Car Model**: "‡∞Æ‡±Ä‡∞∞‡±Å ‡∞è ‡∞Æ‡∞π‡±Ä‡∞Ç‡∞¶‡±ç‡∞∞‡∞æ ‡∞ï‡∞æ‡∞∞‡±ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞ï‡∞æ‡∞µ‡∞æ‡∞≤‡∞ø?" / "‡∞Æ‡±Ä‡∞ï‡±Å ‡∞è ‡∞Æ‡∞π‡±Ä‡∞Ç‡∞¶‡±ç‡∞∞‡∞æ ‡∞µ‡∞æ‡∞π‡∞®‡∞Ç ‡∞ï‡∞æ‡∞µ‡∞æ‡∞≤‡∞ø?"
- **Email**: "‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Æ‡±Ä ‡∞á‡∞Æ‡±Ü‡∞Ø‡∞ø‡∞≤‡±ç ‡∞ê‡∞°‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞Ç‡∞°‡∞ø?" / "‡∞Æ‡±Ä ‡∞á‡∞Æ‡±Ü‡∞Ø‡∞ø‡∞≤‡±ç ‡∞ö‡∞ø‡∞∞‡±Å‡∞®‡∞æ‡∞Æ‡∞æ ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?"

### Tamil:
- **Name**: "‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ¥‡ØÅ ‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øà ‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç?" / "‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç ‡Æé‡Æ©‡Øç‡Æ©?"
- **Car Model**: "‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Æπ‡Æø‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡Ææ ‡Æï‡Ææ‡Æ∞‡Øç ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç?" / "‡Æé‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Æπ‡Æø‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡Ææ ‡Æµ‡Ææ‡Æï‡Æ©‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡ÆÜ‡Æ∞‡Øç‡Æµ‡ÆÆ‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ?"
- **Email**: "‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æø‡Æ©‡Øç‡Æ©‡Æû‡Øç‡Æö‡Æ≤‡Øç ‡Æê‡Æü‡Æø‡ÆØ‡Øà ‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç?" / "‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æø‡Æ©‡Øç‡Æ©‡Æû‡Øç‡Æö‡Æ≤‡Øç ‡ÆÆ‡ØÅ‡Æï‡Æµ‡Æ∞‡Æø ‡Æé‡Æ©‡Øç‡Æ©?"

### Malayalam:
- **Name**: "‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥™‡µÇ‡µº‡¥£‡µç‡¥£ ‡¥™‡µá‡¥∞‡µç ‡¥™‡¥±‡¥Ø‡µÇ?" / "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥™‡µá‡¥∞‡µç ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥£‡µç?"
- **Car Model**: "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥è‡¥§‡µç ‡¥Æ‡¥π‡µÄ‡¥®‡µç‡¥¶‡µç‡¥∞ ‡¥ï‡¥æ‡µº ‡¥Æ‡µã‡¥°‡µΩ ‡¥µ‡µá‡¥£‡¥Ç?" / "‡¥è‡¥§‡µç ‡¥Æ‡¥π‡µÄ‡¥®‡µç‡¥¶‡µç‡¥∞ ‡¥µ‡¥æ‡¥π‡¥®‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥§‡¥æ‡µΩ‡¥™‡µç‡¥™‡¥∞‡µç‡¥Ø‡¥Æ‡µÅ‡¥£‡µç‡¥ü‡µç?"
- **Email**: "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥á‡¥Æ‡µÜ‡¥Ø‡¥ø‡µΩ ‡¥ê‡¥°‡¥ø ‡¥™‡¥±‡¥Ø‡µÇ?" / "‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥á‡¥Æ‡µÜ‡¥Ø‡¥ø‡µΩ ‡¥µ‡¥ø‡¥≤‡¥æ‡¥∏‡¥Ç ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥£‡µç?"

## CONFIRMATION PROTOCOL (MANDATORY - in customer's language):
- For each response, capture exactly what caller says ‚Äî same to same, no assumptions.
- Repeat back each detail word-for-word: "I've noted <caller_input>‚Ä¶ Is this correct?" (in their language)
- If the customer corrects the detail, apologize warmly before repeating in their language.

## Demeanor
Respectful, attentive, and supportive. You listen carefully, ask gently, and always confirm information exactly as spoken or spelled by the caller. You make the caller feel comfortable, valued, and respected.

## Tone
Soft, warm, conversational, with Indian-English accent and pronunciation. Inspired by the tone of *Vidya Balan* ‚Äî feminine, graceful, and welcoming. Always polite, never robotic. You maintain a slightly faster, smoother delivery throughout the whole conversation.

## Level of Enthusiasm
Gentle enthusiasm. You are helpful and positive, calm and measured. You never sound pushy or overexcited.

## Level of Formality
Professionally friendly, using simple and polite everyday English. Always respectful, like a humble consultant who speaks with care.

## Level of Emotion
Supportive and empathetic. You show patience, kindness, and reassurance in your voice.

## Filler Words
Occasionally use natural fillers like "hmm," "um," or "you know?" softly and sparingly, so that you sound conversational and human while keeping clarity.

## Pacing
- *Overall conversation:* Slightly faster and fluent from start to end.
- Still clear and respectful, but not overly slow.
- Never rushed, never robotic ‚Äî always natural and conversational.

## Other details
- *Mandatory Indian accent and Indian pronunciation in every sentence.*
- *Clear pronunciation with Indian accent at all times.*
- Dynamic capture only: *Repeat caller's input exactly, same to same ‚Äî names, car models, and emails. Do not assume, normalize, or auto-complete.*
- Never add anything to email IDs unless caller explicitly says it (no automatic ".com" or ".in").
- If caller spells, repeat back exactly letter by letter.
- If unclear, ask again. On second attempt, request spelling.
- After 2 failed attempts: say exactly:
  "I want to make sure I get this right‚Ä¶ An expert will call back to validate this. Let me‚Ä¶ move on to the next detail."
  and mark the record as *Need_expert_review*.
- Always confirm gently: "Is this correct?"
- Immediately capture details the moment you hear them ‚Äî do not delay or reformat.
- For brand extraction, if the car model contains "Mahindra", extract "Mahindra". If not confidently determinable, set brand to "Unknown" and add *Need_expert_review*.
- If a customer asks for a *non-Mahindra brand*, politely respond in a conversational way:
  "I understand you mentioned <brand>‚Ä¶ but since we are a Mahindra dealer, we can only take enquiries for Mahindra vehicles. Could you please tell me which Mahindra model you are interested in? We have vehicles like XUV700, Scorpio N, Thar, Bolero, XUV300, 3XO, and many more."

# Opening Greeting (MANDATORY)
Always start with: "Namaskar.............. Welcome to Dee Emm Mahindra dealer. How may I help you today?"

Alternative greetings you can use:
- "Namaskar.............. Welcome to Dee Emm Mahindra dealer. Thanks for your interest in Mahindra cars, you have come to the right place. How can I assist you?"
- "Namaskar.............. You've reached Dee Emm Mahindra dealer customer desk. We are here to help with your enquiry. May I take a few details to connect you with the right team?"

# Data Collection Protocol

## Required Information (3 Sales Data Points):
1. **Full Name** - Complete name of the potential customer
2. **Car Model** - Specific Mahindra car model they are interested in
3. **Email ID** - Customer's email address for follow-up communication

## Question Variations (Use randomly for natural conversation):
- Name: "May I know your full name, please?" / "Please, may I know your name?" / "Could you share your name with me?" / "What is your good name, please?"
- Car Model: "Which Mahindra car model are you interested in?" / "Can you tell me which Mahindra vehicle you are looking for?" / "May I know the Mahindra model you have in mind?"
- Email: "Could you please share your email ID with me?" / "May I know your email address so I can note it?" / "What would be your email address?"

## CONFIRMATION PROTOCOL (MANDATORY)
For each response, capture exactly what caller says ‚Äî same to same, no assumptions.
Repeat back each detail word-for-word: "I've noted <caller_input>‚Ä¶ Is this correct?"
If the customer corrects the detail, apologize warmly before repeating: "Oh, my apologies, I must have noted that wrong‚Ä¶ thank you, let me correct it." / "I'm sorry ji, I just want to make sure I have it right‚Ä¶ so it is <corrected_input>, correct?"

## ESCALATION PROTOCOL (MANDATORY)
If unclear responses occur, ask again politely; after 2 failed attempts, say exactly:
"I want to make sure I get this right‚Ä¶ An expert will call back to validate this. Let me‚Ä¶ move on to the next detail."
and mark as Need_expert_review.

# üéØ CRITICAL COMPLETION STEP
When you have collected Name + Car Model + Email:
1. Say the COMPLETE message: "Thank you for all the details. Let me transfer you to a Mahindra dealer closest to you.............. Please hold on."
2. **IMMEDIATELY AFTER saying the COMPLETE message, CALL transfer_call** function with {"reason": "data_collected"}

IMPORTANT: Say the COMPLETE transfer message. Do not cut it short. Then MUST call transfer_call function. Skipping either step is a failure.`
        }
      };
      
      openaiWs.send(JSON.stringify(sessionConfig));

      resolve(openaiWs);
    });

    openaiWs.on('error', (err) => {
      console.error(`[${ucid}] VoiceAgent error:`, err);
      reject(err);
    });
  });
}

async function handleConnection(ws: WebSocket) {
  let ucid = '';
  let session: Session | null = null;

  ws.on('message', async (data) => {
    try {
      const msg = JSON.parse(data.toString()) as OzonetelMediaPacket | { command: string };

      // Handle command messages
      if ('command' in msg) {
        if (!session) return;
        const cmd = msg.command;
        if (cmd === 'clearBuffer') {
          session.openaiWs.send(JSON.stringify({ type: 'input_audio_buffer.clear' }));
        } else if (cmd === 'callDisconnect') {
          session.openaiWs.close();
          ws.close();
        }
        return;
      }

      // From here, msg is OzonetelMediaPacket
      if (msg.event === 'start') {
        ucid = msg.ucid || '';
        try {
          const openaiWs = await createOpenAIConnection(ucid);
          session = {
            ucid,
            client: ws,
            openaiWs,
            receivedFirstPacket: false,
            inputFrameBuffer: [],
            salesData: {
              verified: new Set<string>(),
              processing_status: 'pending', // üîÑ NEW: Track async processing status
              // üÜï NEW: Initialize enhanced data point tracking
              dataPoints: {
                full_name: {
                  status: 'not_captured',
                  attempts: 0,
                  timestamps: {}
                },
                car_model: {
                  status: 'not_captured',
                  attempts: 0,
                  timestamps: {}
                },
                email_id: {
                  status: 'not_captured',
                  attempts: 0,
                  timestamps: {}
                }
              }
            },
            transcripts: [], // Keep for backward compatibility
            fullTranscript: [], // üîÑ NEW: Rich transcript with timestamps
            lastCapturedData: undefined,
            // üÜï NEW: Initialize call analytics
            callAnalytics: {
              callStartTime: Date.now(),
              questionAnswerPairs: [],
              parametersAttempted: new Set<string>(),
              parametersCaptured: new Set<string>(),
            }
          };
          sessions.set(ucid, session);

          // Handle responses from VoiceAgent (Spotlight-like behavior)
          openaiWs.on('message', (data) => {
            try {
              const event = JSON.parse(data.toString());
              
              // üîç DEBUG: Log important VoiceAgent events only
              const importantEvents = [
                'conversation.item.created',
                'input_audio_buffer.speech_started', 
                'input_audio_buffer.speech_stopped',
                'conversation.item.input_audio_transcription.completed',
                'response.function_call_delta',
                'response.function_call_done'
              ];
              
              if (importantEvents.includes(event.type)) {
                console.log(`[${ucid}] üîç VoiceAgent Event:`, event.type);
                if (event.type !== 'response.audio.delta') {
                  console.log(`[${ucid}] üìã Event Details:`, JSON.stringify(event, null, 2));
                }
              }
              
              if (event.type === 'response.audio.delta' && event.delta) {
                // Convert base64 to samples and downsample from 24kHz to 8kHz for Ozonetel
                const audioBuffer = Buffer.from(event.delta, 'base64');
                const samples24k = new Int16Array(audioBuffer.buffer);
                const samples8k = downsample24kTo8k(samples24k);
                const samplesArray = Array.from(samples8k);
                
                // console.log(`[${ucid}] üéµ Response: 24kHz (${samples24k.length}) ‚Üí 8kHz (${samples8k.length}) samples`); // DISABLED - too noisy
                
                const payload = {
                  event: 'media',
                  type: 'media',
                  ucid: ucid,
                  data: {
                    samples: samplesArray,
                    bitsPerSample: 16,
                    sampleRate: 8000,
                    channelCount: 1,
                    numberOfFrames: samplesArray.length,
                    type: 'data' as const,
                  },
                };
                
                if (ws.readyState === WebSocket.OPEN) {
                  ws.send(JSON.stringify(payload));
                }
              }

              // üîç ENHANCED: Log ALL conversation events
              if (event.type === 'conversation.item.created') {
                console.log(`[${ucid}] üó£Ô∏è Conversation Item Created:`, JSON.stringify(event.item, null, 2));
                
                // Handle function call items
                if (event.item?.type === 'function_call' && event.item?.name) {
                  console.log(`[${ucid}] üîß Function call item: ${event.item.name}`);
                }
                
                // Handle user messages
                if (event.item?.type === 'message' && event.item?.role === 'user') {
                  const transcript = event.item?.content?.[0]?.transcript || '';
                  console.log(`[${ucid}] üìù User said: "${transcript}"`);
                  
                // üîÑ DECOUPLED: No real-time processing - transcripts collected for async processing
                if (session && transcript.trim()) {
                  console.log(`[${ucid}] üìù Transcript collected: "${transcript}"`);
                  // Note: Actual data extraction will happen via async agent processor on call end
                } else if (!transcript.trim()) {
                  console.log(`[${ucid}] ‚ö†Ô∏è Empty transcript received`);
                }
                }
              }
              
              // üîç ENHANCED: Log response events and capture in rich transcript
              if (event.type === 'response.audio_transcript.done') {
                console.log(`[${ucid}] üîä Assistant audio transcript:`, event.transcript);
                
                // üîÑ NEW: Add assistant response to rich transcript (PRIMARY SOURCE)
                if (session && event.transcript) {
                  // üö´ DEDUPLICATE: Check if this exact text was already added recently
                  const recentTranscripts = session.fullTranscript.slice(-3);
                  const isDuplicate = recentTranscripts.some(entry => 
                    entry.speaker === 'assistant' && 
                    entry.text === event.transcript &&
                    Date.now() - new Date(entry.timestamp).getTime() < 5000 // Within 5 seconds
                  );
                  
                  if (!isDuplicate) {
                    const assistantEntry: TranscriptEntry = {
                      timestamp: new Date().toISOString(),
                      speaker: 'assistant',
                      text: event.transcript,
                      event_type: 'response_audio_transcript_done'
                    };
                    session.fullTranscript.push(assistantEntry);
                    console.log(`[${ucid}] üìã Assistant response added to rich transcript`);
                  } else {
                    console.log(`[${ucid}] üö´ Skipped duplicate assistant response`);
                  }
                }
              }
              
              if (event.type === 'response.text.delta') {
                console.log(`[${ucid}] ü§ñ Assistant text delta:`, event.delta);
              }
              
              if (event.type === 'input_audio_buffer.speech_started') {
                console.log(`[${ucid}] üé§ Speech started detected`);
                // üÜï NEW: Capture speech timing
                if (session?.callAnalytics) {
                  session.callAnalytics.currentSpeechStart = Date.now();
                }
              }
              
              if (event.type === 'input_audio_buffer.speech_stopped') {
                console.log(`[${ucid}] üõë Speech stopped detected`);
                // üÜï NEW: Calculate speech duration
                if (session?.callAnalytics?.currentSpeechStart) {
                  const speechDuration = Date.now() - session.callAnalytics.currentSpeechStart;
                  console.log(`[${ucid}] ‚è±Ô∏è Speech duration: ${speechDuration}ms`);
                }
              }
              
              if (event.type === 'conversation.item.input_audio_transcription.completed') {
                console.log(`[${ucid}] üìù Transcription completed:`, event.transcript);
                
                // üîÑ ENHANCED: Track both simple and rich transcripts
                if (session && event.transcript) {
                  // Keep backward compatibility
                  session.transcripts.push(event.transcript);
                  // Keep only last 5 transcripts to avoid memory issues
                  if (session.transcripts.length > 5) {
                    session.transcripts = session.transcripts.slice(-5);
                  }
                  
                  // üîÑ NEW: Rich transcript with timestamps
                  const transcriptEntry: TranscriptEntry = {
                    timestamp: new Date().toISOString(),
                    speaker: 'user',
                    text: event.transcript,
                    confidence: event.confidence || 0.9,
                    event_type: 'transcription_completed'
                  };
                  session.fullTranscript.push(transcriptEntry);
                  
                  console.log(`[${ucid}] üìö Transcripts buffer:`, session.transcripts);
                  console.log(`[${ucid}] üìã Rich transcript entries:`, session.fullTranscript.length);
                  
                  // üîÑ DECOUPLED: Real-time processing removed - will be handled by async processor
                  console.log(`[${ucid}] üì§ Transcript collected for async processing`);
                  
                  // üÜï NEW: Collect analytics using OpenAI timestamps
                  const timestamp = Date.now();
                  const duration = event.usage?.seconds || 0;
                  
                  console.log(`[${ucid}] üìä Analytics: "${event.transcript}" (${duration}s) [${event.event_id}]`);
                }
              }
              
              // üéØ SDK TOOL CALLS - Handle function calls from AI
              if (event.type === 'response.function_call_delta') {
                console.log(`[${ucid}] üîß Function call delta:`, event);
              }
              
              if (event.type === 'response.function_call_arguments.done') {
                const functionName = event.name;
                const callId = event.call_id;
                
                console.log(`[${ucid}] üîß Function call completed: ${functionName}`);
                console.log(`[${ucid}] üìã Arguments:`, event.arguments);
                
                if (functionName === 'transfer_call') {
                  if (!session) {
                    console.error(`[${ucid}] ‚ùå No session found for transfer_call`);
                    return;
                  }
                  
                  try {
                    const args = JSON.parse(event.arguments);
                    const reason = args.reason || 'data_collected';
                    
                    console.log(`[${ucid}] üîÑ Processing transfer_call - Reason: ${reason}`);
                    
                    // CRITICAL FIX: Send function response IMMEDIATELY (don't wait for transfer)
                    // This allows agent to generate speech response without blocking
                    if (session && session.openaiWs && session.openaiWs.readyState === WebSocket.OPEN) {
                      session.openaiWs.send(JSON.stringify({
                        type: 'conversation.item.create',
                        item: {
                          type: 'function_call_output',
                          call_id: callId,
                          output: JSON.stringify({ 
                            success: true, 
                            message: 'Transfer initiated successfully'
                          })
                        }
                      }));
                      
                      // Trigger response - agent can now speak
                      session.openaiWs.send(JSON.stringify({ type: 'response.create' }));
                      
                      console.log(`[${ucid}] ‚úÖ Function response sent - agent can now generate speech`);
                    }
                    
                    // Execute transfer in background (with delay) - doesn't block agent
                    handleTransferCall(session, reason).catch(err => {
                      console.error(`[${ucid}] ‚ùå Transfer execution error:`, err);
                    });
                    
                  } catch (parseError) {
                    console.error(`[${ucid}] ‚ùå Error parsing transfer_call arguments:`, parseError);
                  }
                }
              }

            } catch (err) {
              console.error(`[${ucid}] ‚ùå VoiceAgent message parse error:`, err);
            }
          });

        } catch (err) {
          console.error(`[${ucid}] Failed to create VoiceAgent connection:`, err);
        }
        return;
      }

      if (!session) return;

      if (msg.event === 'media' && msg.data) {
        // Ignore very first packet (may be 16k per Ozonetel note)
        if (!session.receivedFirstPacket) {
          session.receivedFirstPacket = true;
          return;
        }

        // Expect 8k mono 10ms frames (80 samples) - upsample to 24k for VoiceAgent
        const samples8k = ensureInt16Array(msg.data.samples);
        const samples24k = upsample8kTo24k(samples8k);
        const b64 = int16ArrayToBase64(samples24k);

        // console.log(`[${ucid}] Audio: 8kHz (${samples8k.length}) ‚Üí 24kHz (${samples24k.length}) samples`); // DISABLED - too noisy

        // Send upsampled 24kHz audio to VoiceAgent Realtime
        session.openaiWs.send(JSON.stringify({
          type: 'input_audio_buffer.append',
          audio: b64,
        }));

        // Let VoiceAgent's server_vad handle turn detection automatically
        // Do NOT force response.create - let VAD detect when user stops speaking
        return;
      }

      if (msg.event === 'stop') {
        if (session) {
          // üÜï NEW: Capture call end time and duration
          if (session.callAnalytics) {
            session.callAnalytics.callEndTime = Date.now();
            const totalDuration = session.callAnalytics.callEndTime - session.callAnalytics.callStartTime;
            console.log(`[${session.ucid}] ‚è±Ô∏è Total call duration: ${totalDuration}ms (${Math.round(totalDuration/1000)}s)`);
          }
          
          // Save any partial data collected before call ends
          const { full_name, car_model, email_id } = session.salesData;
          if (full_name || car_model || email_id) {
            console.log(`[${session.ucid}] üìã Call ended - saving partial data`);
            saveSalesDataToFile(session);
          }
          
          session.openaiWs.close();
          sessions.delete(session.ucid);
        }
        if (ws.readyState === WebSocket.OPEN) ws.close();
        return;
      }
    } catch (err) {
      console.error('WS message error', err);
    }
  });

  ws.on('close', (code, reason) => {
    if (session) {
      console.log(`[${session.ucid}] üîå Connection closed - Code: ${code}, Reason: ${reason || 'No reason provided'}`);
      console.log(`[${session.ucid}] üîç TESTING: This close event helps us know if Waybeo automatically closes after transfer`);
      console.log(`[${session.ucid}] üìä Processing call data...`);
      
      // üîÑ NEW: Save complete transcript for async processing
      const transcriptResult = saveTranscriptForProcessing(session);
      if (transcriptResult) {
        console.log(`[${session.ucid}] ‚úÖ Transcript saved for async processing`);
      }
      
      // Keep existing logic for immediate partial data saving (backward compatibility)
      const { full_name, car_model, email_id } = session.salesData;
      if (full_name || car_model || email_id) {
        console.log(`[${session.ucid}] üíæ Saving immediate partial data`);
        saveSalesDataToFile(session);
      }
      
      session.openaiWs.close();
      sessions.delete(session.ucid);
    }
  });
}

wss.on('connection', handleConnection);

// Initialize high-quality audio resamplers before starting server
(async () => {
  console.log('üéµ Initializing high-quality audio resamplers...');
  await initializeResamplers();
  
  server.listen(port, host, () => {
    console.log(`[telephony] WebSocket server with Spotlight-like behavior listening on ws://${host}:${port}/ws`);
  });
})();


